## 核心思路

### 1. Hash 计算

**文件 Hash（fileHash）**

-   用于文件唯一标识、断点续传恢复、秒传
-   文件选择后立即计算（使用 SparkMD5 增量计算）

**Chunk Hash（chunkHash）**

-   用于验证每个 chunk 数据完整性
-   每个 chunk 上传前计算

### 2. 前端流程

```javascript
// 1. 计算文件 hash
const fileHash = await calculateFileHash(file);

// 2. 检查已上传的 chunk（本地存储或服务器）
const uploadedChunks = await checkUploadedChunks(fileHash);

// 3. 分块上传（跳过已上传的 chunk）
for (let chunkIndex = 0; chunkIndex < totalChunks; chunkIndex++) {
    if (uploadedChunks.has(chunkIndex)) continue;

    const chunk = file.slice(chunkIndex * chunkSize, (chunkIndex + 1) * chunkSize);
    const chunkHash = await calculateChunkHash(chunk);

    await uploadChunk({ chunk, chunkIndex, fileHash, chunkHash });
}

// 4. 所有 chunk 上传完成后，请求合并
await fetch('/upload/merge', { fileHash, filename, totalChunks });
```

### 3. 后端流程

**Chunk 上传接口**

```javascript
app.post('/upload', upload.single('chunk'), (req, res) => {
    // 1. 验证 chunk hash
    if (!verifyChunkHash(chunkBuffer, chunkHash)) {
        return res.status(400).json({ success: false });
    }

    // 2. 存储到 uploads/chunks/{fileHash}/{chunkIndex}.chunk
    const chunkPath = path.join('uploads/chunks', fileHash, `${chunkIndex}.chunk`);
    fs.renameSync(chunkFile.path, chunkPath);

    // 3. 更新上传状态
    uploadStatus.get(fileHash).uploadedChunks.add(chunkIndex);
});
```

**文件合并接口**

```javascript
app.post('/upload/merge', async (req, res) => {
    const { fileHash, totalChunks } = req.body;

    // 1. 验证所有 chunk 是否已上传
    const status = uploadStatus.get(fileHash);
    if (status.uploadedChunks.size !== totalChunks) {
        return res.status(400).json({ success: false });
    }

    // 2. 按顺序合并所有 chunk
    const writeStream = fs.createWriteStream(finalPath);
    for (let i = 0; i < totalChunks; i++) {
        const chunkBuffer = fs.readFileSync(`uploads/chunks/${fileHash}/${i}.chunk`);
        writeStream.write(chunkBuffer);
    }
    writeStream.end();

    // 3. 可选：验证合并后文件 hash
    // 4. 清理 chunk 文件
});
```

## 关键点

### Hash 的作用

1. **文件唯一性**：相同文件产生相同 hash，支持秒传、去重
2. **数据完整性**：chunk hash 验证传输是否损坏
3. **断点续传**：通过 fileHash 识别同一文件，恢复上传进度

### 文件合并要点

1. **按顺序合并**：必须按 chunkIndex 顺序，否则文件损坏
2. **完整性检查**：合并前验证所有 chunk 是否都已上传
3. **Hash 验证**：可选验证合并后文件的完整 hash

### 断点续传恢复

-   **方式 1**：本地存储（LocalStorage），记录 `upload_${fileHash}` 和已上传 chunk 索引
-   **方式 2**：服务器端记录，通过 `/upload/check?hash=${fileHash}` 查询已上传 chunk

## 大文件处理优化

### 1. Hash 计算优化

**问题**：大文件计算 hash 会阻塞主线程，导致页面卡顿

**解决方案**：使用 Web Worker 在后台计算

```javascript
// hash-worker.js
self.importScripts('https://cdn.jsdelivr.net/npm/spark-md5@3.0.2/spark-md5.min.js');

self.onmessage = function (e) {
    const { file, chunkSize } = e.data;
    const spark = new SparkMD5.ArrayBuffer();
    const reader = new FileReader();
    let currentChunk = 0;
    const chunks = Math.ceil(file.size / chunkSize);

    reader.onload = function (event) {
        spark.append(event.target.result);
        currentChunk++;

        if (currentChunk < chunks) {
            loadNext();
        } else {
            const hash = spark.end();
            self.postMessage({ hash });
        }
    };

    function loadNext() {
        const start = currentChunk * chunkSize;
        const end = Math.min(start + chunkSize, file.size);
        reader.readAsArrayBuffer(file.slice(start, end));
    }

    loadNext();
};

// 主线程使用
const worker = new Worker('hash-worker.js');
worker.postMessage({ file, chunkSize: 2 * 1024 * 1024 });
worker.onmessage = (e) => {
    const fileHash = e.data.hash;
    // 继续后续流程
};
```

### 2. 并发控制

**问题**：同时上传大量 chunk 会占用过多网络和内存资源

**解决方案**：限制并发数，使用队列控制

```javascript
class UploadQueue {
    constructor(maxConcurrent = 3) {
        this.maxConcurrent = maxConcurrent;
        this.running = 0;
        this.queue = [];
    }

    async add(task) {
        return new Promise((resolve, reject) => {
            this.queue.push({ task, resolve, reject });
            this.run();
        });
    }

    async run() {
        if (this.running >= this.maxConcurrent || this.queue.length === 0) {
            return;
        }

        this.running++;
        const { task, resolve, reject } = this.queue.shift();

        try {
            const result = await task();
            resolve(result);
        } catch (error) {
            reject(error);
        } finally {
            this.running--;
            this.run(); // 继续处理队列
        }
    }
}

// 使用
const uploadQueue = new UploadQueue(3); // 最多 3 个并发

for (let chunkIndex = 0; chunkIndex < totalChunks; chunkIndex++) {
    if (uploadedChunks.has(chunkIndex)) continue;

    uploadQueue.add(async () => {
        const chunk = file.slice(chunkIndex * chunkSize, (chunkIndex + 1) * chunkSize);
        return await uploadChunk({ chunk, chunkIndex });
    });
}
```

### 3. 内存优化

**关键点**：

-   使用 `File.slice()` 而不是读取整个文件
-   Hash 计算使用增量方式（SparkMD5），不需要一次性加载整个文件
-   上传完的 chunk 及时释放引用

```javascript
// ✅ 正确：分片读取
const chunk = file.slice(start, end);
const hash = await calculateChunkHash(chunk);

// ❌ 错误：一次性读取整个文件
const buffer = await file.arrayBuffer(); // 大文件会导致内存溢出
```

### 4. 进度显示和错误重试

```javascript
let uploadedSize = 0;
const totalSize = file.size;
const retryLimit = 3;

async function uploadChunkWithRetry(chunk, chunkIndex, retries = 0) {
    try {
        await uploadChunk({ chunk, chunkIndex });
        uploadedSize += chunk.size;
        updateProgress((uploadedSize / totalSize) * 100);
    } catch (error) {
        if (retries < retryLimit) {
            await new Promise((resolve) => setTimeout(resolve, 1000 * (retries + 1))); // 指数退避
            return uploadChunkWithRetry(chunk, chunkIndex, retries + 1);
        }
        throw error;
    }
}
```

### 5. 大文件优化策略总结

1. **Hash 计算**：Web Worker 后台计算，避免阻塞主线程
2. **并发控制**：限制同时上传的 chunk 数量（通常 3-5 个）
3. **内存管理**：使用 `File.slice()` 分片处理，不一次性加载
4. **错误重试**：失败自动重试，支持指数退避
5. **进度反馈**：实时更新上传进度，提升用户体验
6. **Chunk 大小**：根据文件大小和网络情况调整（1MB-5MB 较合适）

## 资料

[文章](https://juejin.cn/post/6844904046436843527#heading-25)

[源代码]https://github.com/yeyan1996/file-upload
