# 数据加载与处理（LangChain.js 实践）

在构建基于大模型的 AI 应用时，如何高效地加载、切分和管理文档数据，是实现检索增强（RAG）、知识问答与多轮对话等高级功能的基础。LangChain.js 生态围绕“数据加载与处理”提供了多种工具和组件，包括：

## 1. Document Loaders（文档加载器）

**说明**：Document Loader 用于从各种数据源（如本地文件、网络链接、数据库、第三方 API 等）加载文本或文档，统一输出为可被大模型处理的数据结构。例如，可以加载 Markdown、PDF、TXT、网页等多种格式的内容。

**常见用法举例：**

```js
import { TextLoader } from 'langchain/document_loaders/fs/text';

// 从本地文本文件加载内容
const loader = new TextLoader('./docs/intro.txt');
const docs = await loader.load();
console.log(docs[0].pageContent); // 文档正文
```

除了文本文件，还可以使用 PDFLoader、WebLoader 等分别加载 PDF、网页等内容，实现多源异构数据的统一采集。

## 2. Text Splitters（文本分割器）

**说明**：大模型处理长文本（如书籍、长文档）时，常常需要将其切分成小块，以满足模型输入长度限制并提升检索效率。Text Splitter 支持按字数、句子、段落或自定义规则分块，并可自动生成文档元数据（如分块顺序、位置等）。

**典型用法：**

```js
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';

const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: 500, // 每块字数上限
    chunkOverlap: 50, // 相邻块的内容重叠字数
});

const splitDocs = await splitter.splitDocuments(docs);
console.log(splitDocs.length); // 分块数量
console.log(splitDocs[0].pageContent); // 第一块内容
```

## 3. Vector Stores（向量存储）

**说明**：向量存储（Vector Store）用于保存文档的向量化表示，支持相似度检索。常用场景是实现“基于知识库的问答”或“上下文召回”。LangChain.js 支持多种后端存储（如内存、本地文件、Pinecone、Weaviate、Supabase 等）。

**基本流程：文本 → Embedding → Vector Store**

```js
import { HNSWLib } from 'langchain/vectorstores/hnswlib';
import { OpenAIEmbeddings } from 'langchain/embeddings/openai';

// 生成文档的嵌入向量
const vectorStore = await HNSWLib.fromDocuments(
    splitDocs,
    new OpenAIEmbeddings({ openAIApiKey: process.env.OPENAI_API_KEY })
);

// 相似度搜索
const results = await vectorStore.similaritySearch('什么是文档加载器？', 3);
console.log(results.map((doc) => doc.pageContent));
```

### 为什么要用向量存储？

-   **实现语义检索**：传统数据库只能按关键词精确或模糊匹配，无法理解语义相近的表达。向量存储可通过“向量相似度”快速检索语义相关内容，实现智能问答和知识检索。
-   **支持大规模检索**：向量存储（如 HNSWLib、Pinecone 等）针对高维空间优化，能高效处理成千上万甚至百万级别文档，保证搜索速度和扩展能力。
-   **集成多模态数据**：不仅支持文本，还可用于图片、音频等多种数据类型的嵌入检索，满足 AI 应用的多元需求。
-   **RAG、知识问答基础**：向量存储是“检索增强生成（RAG）”和“知识库问答”方案的核心，实现了“基于语义相似性查找上下文+生成答案”的能力。
-   **易于集成与扩展**：LangChain.js 支持向量存储模块与主流云服务、数据库无缝衔接，便于企业级部署和系统集成。

**简言之：有了向量存储，你才能用 AI 构建真正懂你、会“类比推理”的智能知识检索和问答系统。**

## 4. Embeddings（嵌入向量）

**说明**：将文本编码为向量（Embedding）是语义检索和相关性排序的基础。LangChain.js 集成了 OpenAI、Cohere 等主流嵌入模型接口，支持自定义模型。

**简要用法：**

```js
import { OpenAIEmbeddings } from 'langchain/embeddings/openai';

const embeddings = new OpenAIEmbeddings({ openAIApiKey: 'sk-xxx' });
const vector = await embeddings.embedQuery('什么是向量数据库？');
console.log(vector); // 返回一个向量数组
```

---

## 总结

数据加载与处理是 LangChain.js 构建智能应用的核心环节。文档加载器采集数据，文本分割器处理长文档，嵌入模型将文本转为向量，向量存储实现高效检索。这一串流程是“RAG”、“知识问答”等应用不可或缺的技术基石。熟练掌握这些内容，有助于你开发强大的 AI 产品和工具。
