# 案例 7：日志系统 - 高并发写入设计

## 需求场景

系统需要记录用户操作日志，特点：

-   写入频率高（每秒数千条）
-   查询频率低（主要是统计分析）
-   数据量大（每天百万级）
-   需要按时间范围查询

## 第一次设计（错误）

```sql
CREATE TABLE operation_logs (
    log_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id BIGINT,
    operation_type VARCHAR(50),
    content TEXT,
    ip_address VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_user_id (user_id),
    INDEX idx_created_at (created_at)
);
```

**问题**：

1. 高并发写入时，主键自增锁竞争激烈
2. 单表数据量过大，查询慢
3. 索引过多影响写入性能

## 设计思路

**核心问题**：如何优化高并发写入性能？

**解决方案**：

1. **分表**：按时间分表（按月或按天）
2. **减少索引**：只保留必要的索引
3. **异步写入**：使用消息队列缓冲写入

## 最终设计

```sql
-- 按月分表：operation_logs_202401, operation_logs_202402
CREATE TABLE operation_logs_202401 (
    log_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id BIGINT,
    operation_type VARCHAR(50),
    table_name VARCHAR(50) COMMENT '操作的表名',
    record_id BIGINT COMMENT '操作的记录ID',
    old_value JSON COMMENT '旧值',
    new_value JSON COMMENT '新值',
    ip_address VARCHAR(50),
    user_agent VARCHAR(255),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_user_created (user_id, created_at),
    INDEX idx_table_record (table_name, record_id),
    INDEX idx_created_at (created_at)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
PARTITION BY RANGE (UNIX_TIMESTAMP(created_at)) (
    PARTITION p202401 VALUES LESS THAN (UNIX_TIMESTAMP('2024-02-01')),
    PARTITION p202402 VALUES LESS THAN (UNIX_TIMESTAMP('2024-03-01'))
);
```

## 设计要点

1. **分表策略**：按月分表，查询时路由到对应月份的表
2. **分区表**：MySQL 5.7+ 支持分区，可以进一步优化
3. **减少索引**：只保留查询必需的索引
4. **JSON 字段**：使用 JSON 存储新旧值，灵活但查询性能略差（适合日志场景）

## 实际应用

```sql
-- 写入日志（路由到对应月份的表）
-- 应用层根据当前时间决定写入哪张表
INSERT INTO operation_logs_202401
(user_id, operation_type, table_name, record_id, old_value, new_value, ip_address)
VALUES (?, ?, ?, ?, ?, ?, ?);

-- 查询日志（需要查询多张表）
SELECT * FROM operation_logs_202401
WHERE user_id = ? AND created_at >= '2024-01-01' AND created_at < '2024-02-01'
UNION ALL
SELECT * FROM operation_logs_202402
WHERE user_id = ? AND created_at >= '2024-02-01' AND created_at < '2024-03-01';

-- 或者使用消息队列异步写入
-- 1. 日志写入消息队列（Redis/RabbitMQ）
// producer.send('log_queue', logData);

-- 2. 消费者批量写入数据库
INSERT INTO operation_logs_202401 (...) VALUES (...), (...), (...);
```

## 掌握能力

学习完这个案例，你将掌握：

1. **高并发写入优化**：理解如何通过分表、减少索引、异步写入优化高并发写入性能
2. **分表策略**：掌握按时间分表的策略，以及如何路由查询到正确的表
3. **分区表使用**：了解 MySQL 分区表的使用场景和语法
4. **索引权衡**：理解在写入频繁的场景下，如何权衡索引数量和查询性能
5. **异步处理**：掌握使用消息队列缓冲写入，批量处理提高性能

**应用场景**：日志系统、操作审计、行为追踪、任何高并发写入的场景
